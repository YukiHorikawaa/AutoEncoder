{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEモデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# from torchvision import datasets, transforms\n",
    "import dataset\n",
    "import mainmodel\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_curve\n",
    "from LossFunction import SSIMLoss #自作Loss関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 256)\n",
      "(500, 256)\n",
      "----------------------\n",
      "(100, 256)\n",
      "(100, 256)\n",
      "----------------------\n",
      "(100, 256)\n",
      "100\n",
      "----------------------\n",
      "(500, 256)\n",
      "(100, 256)\n",
      "1148 0\n",
      "rate 0.9\n",
      "data.shape[0]: 500\n",
      "rate 450\n",
      "TrainData (1000, 256, 1, 1, 256)\n",
      "TestData (50, 256)\n",
      "ÄnomalyDta (100, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "Dataset.concat_data(\"sample_data\",500)\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"test\")\n",
    "print(\"----------------------\")\n",
    "Dataset.concat_data(\"sample_test\",100)\n",
    "print(\"----------------------\")\n",
    "data = Dataset.read_savedata(\"sample_test\")\n",
    "print(data.shape[0])\n",
    "print(\"----------------------\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata(\"sample_data\", \"sample_test\", 1000, 256, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make\n",
      "type:x<class 'torch.Tensor'>,recon_data<class 'LossFunction.SSIMLoss'>\n",
      "data:xtensor([[[0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836,\n",
      "          0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836, 0.4836]]],\n",
      "       grad_fn=<SigmoidBackward0>),recon_dataSSIMLoss()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SSIMLoss' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gl/kcbdb7pj0yz8846ytcdrjm140000gn/T/ipykernel_60902/557397106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/lab2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/LAB_LAST/AutoEncoder/AutoEncoder/LossFunction.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(recon_data, x, window, alfa, beta, gamma, c1, c2)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type:x{},recon_data{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data:x{},recon_data{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecon_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mrecon_data\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mrecon_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/lab2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SSIMLoss' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "\n",
    "losslist=[]\n",
    "\n",
    "model = mainmodel.Autoencoder2()\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = SSIMLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "\n",
    "outputs = []\n",
    "#テンソル型に変換\n",
    "data = torch.from_numpy(data.astype(np.float32)).clone()\n",
    "for epoch in data:\n",
    "    for data in epoch:\n",
    "        recon = model(data)\n",
    "        print(\"type:recon_data{}\".format(type(recon)))\n",
    "        loss = criterion(recon, data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#     print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "    print('Loss:{:.4f}'.format(float(loss)))\n",
    "    losslist.append(loss)\n",
    "    outputs.append((epoch, data, recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"newmodel\"\n",
    "ModelEdit = mainmodel.Modeledit(\"syuron\")\n",
    "ModelEdit.save_model(model, model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 5\n",
    "\n",
    "for i in range(anomaly_data.shape[0]):\n",
    "    if i % 10 == 0:\n",
    "        data0 =  torch.from_numpy((test_data[i]).astype(np.float32)).clone()\n",
    "        test0 =  torch.from_numpy((anomaly_data[i]).astype(np.float32)).clone()\n",
    "        print(type(data0))\n",
    "        plt.figure(figsize=(9, 2))\n",
    "        # data = data0.numpy()\n",
    "        data = data0[np.newaxis, np.newaxis, :]\n",
    "        recon = model(data).detach().numpy()\n",
    "        plt.plot(data0.numpy())\n",
    "        plt.plot(recon.flatten())\n",
    "        plt.show()\n",
    "        \n",
    "        anomalypoint=sum(abs(recon.flatten()-data0.numpy()))\n",
    "        print(\"normal:\"+str(anomalypoint))\n",
    "        if(anomalypoint>10):\n",
    "            print(\"anomaly\")\n",
    "            \n",
    "        plt.figure(figsize=(9, 2))\n",
    "        # data = test0.numpy()\n",
    "        data = test0[np.newaxis, np.newaxis, :]\n",
    "        recon = model(data).detach().numpy()\n",
    "        plt.plot(test0.numpy(),label=\"testdata\")\n",
    "        plt.plot(recon.flatten(),label=\"recondata\")\n",
    "        plt.show()\n",
    "        \n",
    "        anomalypoint=sum(abs(recon.flatten()-test0.numpy()))\n",
    "        print(\"anomarly:\"+str(anomalypoint))\n",
    "        if(anomalypoint>10):\n",
    "            print(\"anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oc-svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# from torchvision import datasets, transforms\n",
    "import dataset\n",
    "import mainmodel\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata_ocs(\"sample_data\", \"sample_test\", 1000, 256, 1)\n",
    "\n",
    "input_model = mainmodel.Autoencoder2()\n",
    "input_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_model = mainmodel.Autoencoder2().to(device)\n",
    "input_model.load_state_dict(torch.load(\"/Users/yukihorikawa/Desktop/LAB_LAST/AutoEncoder/program/syuron/model_data/20211206/newmodel.pth\", map_location=device))\n",
    "\n",
    "# input_model = model\n",
    "\n",
    "recon_list, encoded_list, input_list = dataloader.ocsvm_dataset(input_model, data)\n",
    "test_recon_list, test_encoded_list, test_input_list = dataloader.ocsvm_dataset(input_model, test_data)\n",
    "anomaly_recon_list, anomaly_encoded_list, anomaly_input_list = dataloader.ocsvm_dataset(input_model, anomaly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "X_anomaly = anomaly_encoded_list\n",
    "X_test = test_encoded_list\n",
    "X_train = encoded_list\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from skimage import measure\n",
    "from sklearn import svm\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "SPACE_SAMPLING_POINTS = 100\n",
    "TRAIN_POINTS = 100\n",
    "\n",
    "# Define the size of the space which is interesting for the example\n",
    "X_MIN = -100\n",
    "X_MAX = 100\n",
    "Y_MIN = -100\n",
    "Y_MAX = 100\n",
    "Z_MIN = -100\n",
    "Z_MAX = 100\n",
    "\n",
    "# Generate a regular grid to sample the 3D space for various operations later\n",
    "xx, yy, zz = np.meshgrid(np.linspace(X_MIN, X_MAX, SPACE_SAMPLING_POINTS),\n",
    "                        np.linspace(Y_MIN, Y_MAX, SPACE_SAMPLING_POINTS),\n",
    "                        np.linspace(Z_MIN, Z_MAX, SPACE_SAMPLING_POINTS))\n",
    "\n",
    "# Generate some abnormal novel observations using a different distribution\n",
    "X_outliers = np.random.uniform(low=-10, high=10, size=(20, 3))\n",
    "\n",
    "# Create a OneClassSVM instance and fit it to the data\n",
    "clf = svm.OneClassSVM(nu=0.009, kernel=\"rbf\", gamma=0.8)\n",
    "clf.fit(X_train)\n",
    "# Predict the class of the various input creaxted before\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_anomaly = clf.predict(X_anomaly)\n",
    "# y_pred_normal = clf.predict(X_anomaly)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "print(\"y_pred_train:{} \".format(y_pred_train.shape[0] ))\n",
    "print(\"y_pred_anomaly:{} \".format(y_pred_anomaly ))\n",
    "print(\"y_pred_outliers:{} \".format(y_pred_outliers ))\n",
    "# And compute classification error frequencies\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_anomaly = y_pred_anomaly[y_pred_anomaly == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "error_train = y_pred_train[y_pred_train == -1]\n",
    "error_anomaly = y_pred_anomaly[y_pred_anomaly == -1]\n",
    "error_test = y_pred_test[y_pred_test == 1]\n",
    "# Calculate the distance from the separating hyperplane of the SVM for the\n",
    "# whole space using the grid defined in the beginning\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel(), zz.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create a figure with axes for 3D plotting\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "fig.suptitle(\"Novelty Detection\")\n",
    "\n",
    "# Plot the different input points using 3D scatter plotting\n",
    "b1 = ax.scatter(X_train[:, 0], X_train[:, 1], X_train[:, 2], c='white', alpha=0.5)\n",
    "# ax.scatter(error_train[:, 0],error_train[:, 1], error_train[:, 2], c='black', alpha=0.5)\n",
    "b2 = ax.scatter(X_anomaly[:, 0], X_anomaly[:, 1], X_anomaly[:, 2], c='yellow', alpha=0.5)\n",
    "# ax.scatter(error_anomaly[:, 0],error_anomaly[:, 1], error_anomaly[:, 2], c='red', alpha=0.5)\n",
    "c = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c='cyan', alpha=0.5)\n",
    "# ax.scatter(error_test[:, 0],error_test[:, 1], error_test[:, 2], c='magenta', alpha=0.5)\n",
    "# Plot the separating hyperplane by recreating the isosurface for the distance\n",
    "# == 0 level in the distance grid computed through the decision function of the\n",
    "# SVM. This is done using the marching cubes algorithm implementation from\n",
    "# scikit-image.\n",
    "# verts, faces = measure.marching_cubes(Z, 0)\n",
    "# # Scale and transform to actual size of the interesting volume\n",
    "# verts = verts * \\\n",
    "#     [X_MAX - X_MIN, Y_MAX - Y_MIN, Z_MAX - Z_MIN] / SPACE_SAMPLING_POINTS\n",
    "# verts = verts + [X_MIN, Y_MIN, Z_MIN]\n",
    "# # and create a mesh to display\n",
    "# mesh = Poly3DCollection(verts[faces],\n",
    "#                         facecolor='orange', edgecolor='gray', alpha=0.3)\n",
    "# ax.add_collection3d(mesh)\n",
    "\n",
    "# # Some presentation tweaks\n",
    "# ax.set_xlim((-5, 5))\n",
    "# ax.set_ylim((-5, 5))\n",
    "# ax.set_zlim((-5, 5))\n",
    "\n",
    "# ax.set_xlabel(\"X\")\n",
    "# ax.set_ylabel(\"Y\")\n",
    "# ax.set_zlabel(\"Z\")\n",
    "# ax.legend([mpatches.Patch(color='orange', alpha=0.3), b1, b2, c],\n",
    "#     [\"learned frontier\", \"training observations\",\n",
    "#     \"new regular observations\", \"new abnormal observations\"],\n",
    "#     loc=\"lower left\",\n",
    "#     prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "ax.set_title(\n",
    "    \"error train: %d/%d ; errors anomaly: %d/%d ; \"\n",
    "    \"error test: %d/%d \"\n",
    "    % (n_error_train, X_train.shape[0], n_error_anomaly, X_anomaly.shape[0], n_error_test, X_test.shape[0]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "scores = ['precision', 'recall']\n",
    "nus = [0.001, 0.01, 0.1, 0.002, 0.02, 0.2, 1]\n",
    "gammas = [0.001, 0.01, 0.1, 0.002, 0.02, 0.2, 1]\n",
    "tuned_parameters = {'kernel' : ['rbf'], 'gamma' : gammas, 'nu': nus}\n",
    "for score in scores:\n",
    "    clf = GridSearchCV(svm.OneClassSVM(), tuned_parameters, cv=10,\n",
    "                        scoring='%s_macro' % score, return_train_score=True)\n",
    "\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    resultDf = pd.DataFrame(clf.cv_results_)\n",
    "    print(resultDf[[\"mean_test_score\", \"std_test_score\", \"params\"]].sort_values(by=[\"mean_test_score\"], ascending=False).head())\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_anomaly = anomaly_encoded_list\n",
    "X_test = test_encoded_list\n",
    "X_train = encoded_list\n",
    "print(\"X_anomaly:{}, X_test:{}, X_train{}\".format(X_anomaly.shape[0], X_test.shape[0], X_train.shape[0]))\n",
    "span = 0.001\n",
    "nus = np.arange(span, 0.01+span, span, dtype = 'float64')\n",
    "print(nus)\n",
    "gammas = np.arange(span, 0.01+span, span, dtype = 'float64')\n",
    "point_test = X_test.shape[0]\n",
    "point_test_def = 0\n",
    "poit_error = 0\n",
    "poit_error_def = X_anomaly.shape[0]\n",
    "standar_point = 5\n",
    "parameter = []\n",
    "\n",
    "\n",
    "nus = [0.001, 0.01, 0.1, 0.002, 0.02, 0.2, 0.003, 0.03, 0.3, 0.004, 0.04, 0.4, 0.005, 0.05, 0.5, 0.006, 0.06, 0.6, 0.007, 0.07, 0.7, 0.008, 0.08, 0.8, 0.009, 0.09, 0.9, 1]\n",
    "gammas = [0.001, 0.01, 0.1, 0.002, 0.02, 0.2, 0.003, 0.03, 0.3, 0.004, 0.04, 0.4, 0.005, 0.05, 0.5, 0.006, 0.06, 0.6, 0.007, 0.07, 0.7, 0.008, 0.08, 0.8, 0.009, 0.09, 0.9, 1]\n",
    "num = len(nus)\n",
    "\n",
    "for i in range(num):\n",
    "    for j in range(num):\n",
    "        clf = svm.OneClassSVM(nu=nus[i], kernel=\"rbf\", gamma=gammas[j])\n",
    "        clf.fit(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        y_pred_anomaly = clf.predict(X_anomaly)\n",
    "        n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "        n_error_anomaly = y_pred_anomaly[y_pred_anomaly == -1].size\n",
    "        print(\"n_error_test{}\".format(n_error_test))\n",
    "        print(\"n_error_anomaly{}\".format(n_error_anomaly))\n",
    "        print(\"poit_error_def - n_error_anomaly{}\".format(poit_error_def - n_error_anomaly))\n",
    "        if standar_point > n_error_test + (poit_error_def - n_error_anomaly):\n",
    "            parameter.append([nus[i], gammas[j], n_error_test+poit_error_def - n_error_anomaly])\n",
    "            # standar_point = point_test + (poit_error_def - poit_error)\n",
    "        # if poit_error < n_error_anomaly:\n",
    "        #     poit_error = n_error_anomaly\n",
    "        #     if point_test > n_error_test:\n",
    "        #         point_test = n_error_test\n",
    "        #         print(\"y_pred_test:{} \".format(point_test ))\n",
    "        #         print(\"y_pred_anomaly:{} \".format(poit_error ))\n",
    "        #         parameter.append([nus[i], gammas[j]])\n",
    "        #         print(\"prameter{}\".format(parameter))\n",
    "                # standar_point = point_test + (poit_error_def - poit_error)\n",
    "print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(anomaly_data.shape[0]):\n",
    "    # if i % 10 == 0:\n",
    "    data0 =  torch.from_numpy((test_data[i]).astype(np.float32)).clone()\n",
    "    test0 =  torch.from_numpy((anomaly_data[i]).astype(np.float32)).clone()\n",
    "    print(type(data0))\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    # data = data0.numpy()\n",
    "    data = data0[np.newaxis, np.newaxis, :]\n",
    "    recon = input_model(data).detach().numpy()\n",
    "    plt.plot(data0.numpy())\n",
    "    plt.plot(recon.flatten())\n",
    "    plt.show()\n",
    "    \n",
    "    anomalypoint=sum(abs(recon.flatten()-data0.numpy()))\n",
    "    print(\"normal:\"+str(anomalypoint))\n",
    "    if(anomalypoint>10):\n",
    "        print(\"anomaly\")\n",
    "        \n",
    "    plt.figure(figsize=(9, 2))\n",
    "    # data = test0.numpy()\n",
    "    data = test0[np.newaxis, np.newaxis, :]\n",
    "    recon = input_model(data).detach().numpy()\n",
    "    plt.plot(test0.numpy(),label=\"testdata\")\n",
    "    plt.plot(recon.flatten(),label=\"recondata\")\n",
    "    plt.show()\n",
    "    \n",
    "    anomalypoint=sum(abs(recon.flatten()-test0.numpy()))\n",
    "    print(\"anomarly:\"+str(anomalypoint))\n",
    "    if(anomalypoint>10):\n",
    "        print(\"anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 時間計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from skimage import measure\n",
    "from sklearn import svm\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata_ocs(\"sample_data\", \"sample_test\", 1000, 256, 1)\n",
    "\n",
    "input_model = mainmodel.Autoencoder2()\n",
    "input_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_model = mainmodel.Autoencoder2().to(device)\n",
    "input_model.load_state_dict(torch.load(\"/Users/yukihorikawa/Desktop/LAB_LAST/AutoEncoder/program/syuron/model_data/20211206/newmodel.pth\", map_location=device))\n",
    "\n",
    "# input_model = model\n",
    "\n",
    "recon_list, encoded_list, input_list = dataloader.ocsvm_dataset(input_model, data)\n",
    "test_recon_list, test_encoded_list, test_input_list = dataloader.ocsvm_dataset(input_model, test_data)\n",
    "anomaly_recon_list, anomaly_encoded_list, anomaly_input_list = dataloader.ocsvm_dataset(input_model, anomaly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "\n",
    "#---------for AE------------\n",
    "input_model = mainmodel.Autoencoder2()\n",
    "input_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_model = mainmodel.Autoencoder2().to(device)\n",
    "input_model.load_state_dict(torch.load(\"/Users/yukihorikawa/Desktop/LAB_LAST/AutoEncoder/program/syuron/model_data/20211206/newmodel.pth\", map_location=device))\n",
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata(\"sample_data\", \"sample_test\", 1000, 256, 1)\n",
    "\n",
    "#---------for OCSVM------------\n",
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "ocs_data, ocs_test_data , ocs_anomaly_data= Dataset.read_traindata_ocs(\"sample_data\", \"sample_test\", 1000, 256, 1)\n",
    "recon_list, encoded_list, input_list = dataloader.ocsvm_dataset(input_model, ocs_data)\n",
    "test_recon_list, test_encoded_list, test_input_list = dataloader.ocsvm_dataset(input_model, ocs_test_data)\n",
    "anomaly_recon_list, anomaly_encoded_list, anomaly_input_list = dataloader.ocsvm_dataset(input_model, ocs_anomaly_data)\n",
    "\n",
    "ocs_X_anomaly = anomaly_encoded_list\n",
    "ocs_X_test = test_encoded_list\n",
    "ocs_X_train = encoded_list\n",
    "clf = svm.OneClassSVM(nu=0.009, kernel=\"rbf\", gamma=0.8)\n",
    "clf.fit(ocs_X_train)\n",
    "\n",
    "#---------計測用---------\n",
    "num = 100\n",
    "len = ocs_X_train.shape[0]\n",
    "# ocs_X_train = X_train[np.newaxis, :]#２次元のサイズでないとエラー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata_ocs(\"sample_data\", \"sample_test\", 1000, 256, 1)\n",
    "\n",
    "input_model = mainmodel.Autoencoder2()\n",
    "input_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_model = mainmodel.Autoencoder2().to(device)\n",
    "input_model.load_state_dict(torch.load(\"/Users/yukihorikawa/Desktop/LAB_LAST/AutoEncoder/program/syuron/model_data/20211206/newmodel.pth\", map_location=device))\n",
    "\n",
    "# input_model = model\n",
    "\n",
    "recon_list, encoded_list, input_list = dataloader.ocsvm_dataset(input_model, data)\n",
    "test_recon_list, test_encoded_list, test_input_list = dataloader.ocsvm_dataset(input_model, test_data)\n",
    "anomaly_recon_list, anomaly_encoded_list, anomaly_input_list = dataloader.ocsvm_dataset(input_model, anomaly_data)\n",
    "X_train = encoded_list\n",
    "X_train = X_train[np.newaxis, :]#２次元のサイズでないとエラー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocs_X_train.shape\n",
    "ocs_X_train = X_train[np.newaxis, :]#２次元のサイズでないとエラー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num = 100\n",
    "len = X_train.shape[0]\n",
    "for i in range(num):\n",
    "    for i in range(len):\n",
    "        y_pred_train = clf.predict(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata(\"sample_data\", \"sample_test\", 1000, 256, 1)\n",
    "data =  torch.from_numpy((data).astype(np.float32)).clone()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "num = 100\n",
    "for i in range(num):\n",
    "    for i in range(len):\n",
    "        y_pred_train = input_model(data[i]).detach().numpy()\n",
    "\n",
    "\n",
    "# for i in range(anomaly_data.shape[0]):\n",
    "#     if i % 10 == 0:\n",
    "#         data0 =  torch.from_numpy((test_data[i]).astype(np.float32)).clone()\n",
    "#         test0 =  torch.from_numpy((anomaly_data[i]).astype(np.float32)).clone()\n",
    "#         print(type(data0))\n",
    "#         plt.figure(figsize=(9, 2))\n",
    "#         # data = data0.numpy()\n",
    "#         data = data0[np.newaxis, np.newaxis, :]\n",
    "#         recon = model(data).detach().numpy()\n",
    "#         plt.plot(data0.numpy())\n",
    "#         plt.plot(recon.flatten())\n",
    "#         plt.show()\n",
    "        \n",
    "#         anomalypoint=sum(abs(recon.flatten()-data0.numpy()))\n",
    "#         print(\"normal:\"+str(anomalypoint))\n",
    "#         if(anomalypoint>10):\n",
    "#             print(\"anomaly\")\n",
    "            \n",
    "#         plt.figure(figsize=(9, 2))\n",
    "#         # data = test0.numpy()\n",
    "#         data = test0[np.newaxis, np.newaxis, :]\n",
    "#         recon = model(data).detach().numpy()\n",
    "#         plt.plot(test0.numpy(),label=\"testdata\")\n",
    "#         plt.plot(recon.flatten(),label=\"recondata\")\n",
    "#         plt.show()\n",
    "        \n",
    "#         anomalypoint=sum(abs(recon.flatten()-test0.numpy()))\n",
    "#         print(\"anomarly:\"+str(anomalypoint))\n",
    "#         if(anomalypoint>10):\n",
    "#             print(\"anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5f3f9be2ca8d53a2d9e5a69008919cdcaa5ea3bae845fdf455c7915f0fd9345"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('lab2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
