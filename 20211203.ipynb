{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# from torchvision import datasets, transforms\n",
    "import dataset\n",
    "import mainmodel\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 256)\n",
      "(500, 256)\n",
      "----------------------\n",
      "(100, 256)\n",
      "(100, 256)\n",
      "----------------------\n",
      "(100, 256)\n",
      "100\n",
      "----------------------\n",
      "(500, 256)\n",
      "(100, 256)\n",
      "1148 0\n",
      "rate 0.9\n",
      "data.shape[0]: 500\n",
      "rate 450\n",
      "TrainData (1000, 256, 1, 1, 256)\n",
      "TestData (50, 256)\n",
      "ÄnomalyDta (100, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "Dataset.concat_data(\"sample_data\",500)\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"test\")\n",
    "print(\"----------------------\")\n",
    "Dataset.concat_data(\"sample_test\",100)\n",
    "print(\"----------------------\")\n",
    "data = Dataset.read_savedata(\"sample_test\")\n",
    "print(data.shape[0])\n",
    "print(\"----------------------\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata(\"sample_data\", \"sample_test\", 1000, 256, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0160\n",
      "Loss:0.0064\n",
      "Loss:0.0027\n",
      "Loss:0.0052\n",
      "Loss:0.0045\n",
      "Loss:0.0029\n",
      "Loss:0.0031\n",
      "Loss:0.0019\n",
      "Loss:0.0015\n",
      "Loss:0.0016\n",
      "Loss:0.0030\n",
      "Loss:0.0021\n",
      "Loss:0.0024\n",
      "Loss:0.0017\n",
      "Loss:0.0027\n",
      "Loss:0.0035\n",
      "Loss:0.0011\n",
      "Loss:0.0019\n",
      "Loss:0.0016\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0014\n",
      "Loss:0.0009\n",
      "Loss:0.0021\n",
      "Loss:0.0009\n",
      "Loss:0.0013\n",
      "Loss:0.0013\n",
      "Loss:0.0013\n",
      "Loss:0.0006\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0011\n",
      "Loss:0.0014\n",
      "Loss:0.0013\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0053\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0006\n",
      "Loss:0.0005\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0016\n",
      "Loss:0.0006\n",
      "Loss:0.0011\n",
      "Loss:0.0005\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0013\n",
      "Loss:0.0011\n",
      "Loss:0.0004\n",
      "Loss:0.0016\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.2158\n",
      "Loss:0.0032\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0232\n",
      "Loss:0.0008\n",
      "Loss:0.0018\n",
      "Loss:0.0007\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0018\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0015\n",
      "Loss:0.0011\n",
      "Loss:0.0015\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0222\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0014\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0016\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0005\n",
      "Loss:0.0015\n",
      "Loss:0.0020\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.2008\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0014\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0014\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0004\n",
      "Loss:0.0005\n",
      "Loss:0.0012\n",
      "Loss:0.0009\n",
      "Loss:0.0005\n",
      "Loss:0.0018\n",
      "Loss:0.0014\n",
      "Loss:0.0006\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0015\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0005\n",
      "Loss:0.0006\n",
      "Loss:0.0006\n",
      "Loss:0.0003\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0021\n",
      "Loss:0.0013\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0013\n",
      "Loss:0.0009\n",
      "Loss:0.0013\n",
      "Loss:0.0009\n",
      "Loss:0.0012\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0011\n",
      "Loss:0.0012\n",
      "Loss:0.0003\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0068\n",
      "Loss:0.0013\n",
      "Loss:0.0014\n",
      "Loss:0.0009\n",
      "Loss:0.0004\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0014\n",
      "Loss:0.0011\n",
      "Loss:0.0003\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0005\n",
      "Loss:0.0009\n",
      "Loss:0.0017\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0015\n",
      "Loss:0.0005\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0018\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0023\n",
      "Loss:0.0008\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0007\n",
      "Loss:0.0013\n",
      "Loss:0.0019\n",
      "Loss:0.0184\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0005\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0014\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0014\n",
      "Loss:0.0097\n",
      "Loss:0.0012\n",
      "Loss:0.0009\n",
      "Loss:0.0004\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0004\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0027\n",
      "Loss:0.0012\n",
      "Loss:0.0007\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0008\n",
      "Loss:0.0012\n",
      "Loss:0.0025\n",
      "Loss:0.0013\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0012\n",
      "Loss:0.0010\n",
      "Loss:0.0004\n",
      "Loss:0.0015\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0003\n",
      "Loss:0.0010\n",
      "Loss:0.0095\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0013\n",
      "Loss:0.0004\n",
      "Loss:0.0017\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0009\n",
      "Loss:0.0203\n",
      "Loss:0.0004\n",
      "Loss:0.0012\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0013\n",
      "Loss:0.0065\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0018\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0005\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0006\n",
      "Loss:0.0015\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0012\n",
      "Loss:0.0006\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0018\n",
      "Loss:0.0008\n",
      "Loss:0.0013\n",
      "Loss:0.0013\n",
      "Loss:0.0007\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0003\n",
      "Loss:0.0008\n",
      "Loss:0.0015\n",
      "Loss:0.0006\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0120\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0006\n",
      "Loss:0.0017\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0004\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0003\n",
      "Loss:0.0013\n",
      "Loss:0.0017\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0005\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0004\n",
      "Loss:0.0005\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0005\n",
      "Loss:0.0010\n",
      "Loss:0.0015\n",
      "Loss:0.0012\n",
      "Loss:0.0004\n",
      "Loss:0.0007\n",
      "Loss:0.0563\n",
      "Loss:0.0016\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0003\n",
      "Loss:0.0006\n",
      "Loss:0.0004\n",
      "Loss:0.0011\n",
      "Loss:0.0017\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0011\n",
      "Loss:0.0015\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0094\n",
      "Loss:0.0007\n",
      "Loss:0.0004\n",
      "Loss:0.0014\n",
      "Loss:0.0008\n",
      "Loss:0.0023\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0004\n",
      "Loss:0.0002\n",
      "Loss:0.0137\n",
      "Loss:0.0012\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0015\n",
      "Loss:0.0010\n",
      "Loss:0.0017\n",
      "Loss:0.0009\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0014\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0005\n",
      "Loss:0.0004\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0048\n",
      "Loss:0.0005\n",
      "Loss:0.0005\n",
      "Loss:0.0019\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0022\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0010\n",
      "Loss:0.0003\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0023\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0003\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0013\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0023\n",
      "Loss:0.0009\n",
      "Loss:0.0016\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0004\n",
      "Loss:0.0019\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0014\n",
      "Loss:0.0124\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0015\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0014\n",
      "Loss:0.0004\n",
      "Loss:0.0004\n",
      "Loss:0.0009\n",
      "Loss:0.0014\n",
      "Loss:0.0009\n",
      "Loss:0.0012\n",
      "Loss:0.0005\n",
      "Loss:0.0003\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0014\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0015\n",
      "Loss:0.0013\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0016\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0005\n",
      "Loss:0.0010\n",
      "Loss:0.0022\n",
      "Loss:0.0114\n",
      "Loss:0.0012\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0003\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0005\n",
      "Loss:0.0003\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0018\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0013\n",
      "Loss:0.0014\n",
      "Loss:0.0070\n",
      "Loss:0.0017\n",
      "Loss:0.0008\n",
      "Loss:0.0013\n",
      "Loss:0.0003\n",
      "Loss:0.0007\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0025\n",
      "Loss:0.0009\n",
      "Loss:0.0047\n",
      "Loss:0.0012\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0004\n",
      "Loss:0.0003\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0018\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0003\n",
      "Loss:0.0012\n",
      "Loss:0.0003\n",
      "Loss:0.0003\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0023\n",
      "Loss:0.0008\n",
      "Loss:0.0012\n",
      "Loss:0.0007\n",
      "Loss:0.0013\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0003\n",
      "Loss:0.0008\n",
      "Loss:0.0005\n",
      "Loss:0.0010\n",
      "Loss:0.0003\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0018\n",
      "Loss:0.0034\n",
      "Loss:0.0004\n",
      "Loss:0.0010\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0003\n",
      "Loss:0.0006\n",
      "Loss:0.0004\n",
      "Loss:0.0013\n",
      "Loss:0.0013\n",
      "Loss:0.0008\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0014\n",
      "Loss:0.0005\n",
      "Loss:0.0005\n",
      "Loss:0.0012\n",
      "Loss:0.0014\n",
      "Loss:0.0010\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0095\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0035\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0015\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0013\n",
      "Loss:0.0007\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0013\n",
      "Loss:0.0010\n",
      "Loss:0.0387\n",
      "Loss:0.0007\n",
      "Loss:0.0006\n",
      "Loss:0.0004\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0003\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0006\n",
      "Loss:0.0008\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0019\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0016\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0005\n",
      "Loss:0.0015\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0014\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0005\n",
      "Loss:0.0013\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0014\n",
      "Loss:0.0111\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0005\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0081\n",
      "Loss:0.0008\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0006\n",
      "Loss:0.0029\n",
      "Loss:0.0009\n",
      "Loss:0.0003\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0015\n",
      "Loss:0.0008\n",
      "Loss:0.0003\n",
      "Loss:0.0010\n",
      "Loss:0.0006\n",
      "Loss:0.0006\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0013\n",
      "Loss:0.0006\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0005\n",
      "Loss:0.0013\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0013\n",
      "Loss:0.0005\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0016\n",
      "Loss:0.0010\n",
      "Loss:0.0012\n",
      "Loss:0.0007\n",
      "Loss:0.0005\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0005\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0032\n",
      "Loss:0.0013\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0014\n",
      "Loss:0.0018\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0005\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0016\n",
      "Loss:0.0008\n",
      "Loss:0.0013\n",
      "Loss:0.0006\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0107\n",
      "Loss:0.0015\n",
      "Loss:0.0008\n",
      "Loss:0.0028\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0013\n",
      "Loss:0.0007\n",
      "Loss:0.0005\n",
      "Loss:0.0020\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0015\n",
      "Loss:0.0011\n",
      "Loss:0.0022\n",
      "Loss:0.0007\n",
      "Loss:0.0011\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0005\n",
      "Loss:0.0006\n",
      "Loss:0.0007\n",
      "Loss:0.0012\n",
      "Loss:0.0005\n",
      "Loss:0.0009\n",
      "Loss:0.0003\n",
      "Loss:0.0030\n",
      "Loss:0.0025\n",
      "Loss:0.0009\n",
      "Loss:0.0023\n",
      "Loss:0.0004\n",
      "Loss:0.0012\n",
      "Loss:0.0008\n",
      "Loss:0.0011\n",
      "Loss:0.0006\n",
      "Loss:0.0004\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0012\n",
      "Loss:0.0013\n",
      "Loss:0.0007\n",
      "Loss:0.0009\n",
      "Loss:0.0007\n",
      "Loss:0.0007\n",
      "Loss:0.0010\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0102\n",
      "Loss:0.0010\n",
      "Loss:0.0013\n",
      "Loss:0.0013\n",
      "Loss:0.0010\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0006\n",
      "Loss:0.0009\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n",
      "Loss:0.0005\n",
      "Loss:0.0010\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0005\n",
      "Loss:0.0007\n",
      "Loss:0.0004\n",
      "Loss:0.0011\n",
      "Loss:0.0007\n",
      "Loss:0.0008\n",
      "Loss:0.0009\n",
      "Loss:0.0019\n",
      "Loss:0.0013\n",
      "Loss:0.0009\n",
      "Loss:0.0009\n",
      "Loss:0.0005\n",
      "Loss:0.0011\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0008\n",
      "Loss:0.0008\n",
      "Loss:0.0010\n",
      "Loss:0.0010\n",
      "Loss:0.0007\n",
      "Loss:0.0011\n",
      "Loss:0.0009\n",
      "Loss:0.0010\n",
      "Loss:0.0011\n",
      "Loss:0.0008\n",
      "Loss:0.0012\n",
      "Loss:0.0012\n",
      "Loss:0.0009\n",
      "Loss:0.0004\n",
      "Loss:0.0008\n",
      "Loss:0.0007\n"
     ]
    }
   ],
   "source": [
    "\n",
    "losslist=[]\n",
    "\n",
    "model = mainmodel.Autoencoder2()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "\n",
    "outputs = []\n",
    "#テンソル型に変換\n",
    "data = torch.from_numpy(data.astype(np.float32)).clone()\n",
    "for epoch in data:\n",
    "    for data in epoch:\n",
    "        recon = model(data)\n",
    "#         print(recon.shape, data.shape)\n",
    "        loss = criterion(recon, data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#     print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "    print('Loss:{:.4f}'.format(float(loss)))\n",
    "    losslist.append(loss)\n",
    "    outputs.append((epoch, data, recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"newmodel\"\n",
    "ModelEdit = mainmodel.Modeledit(\"syuron\")\n",
    "ModelEdit.save_model(model, model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 256)\n",
      "(500, 256)\n",
      "----------------------\n",
      "(100, 256)\n",
      "(100, 256)\n",
      "----------------------\n",
      "(100, 256)\n",
      "100\n",
      "----------------------\n",
      "(500, 256)\n",
      "(100, 256)\n",
      "1148 0\n",
      "rate 0.9\n",
      "data.shape[0]: 500\n",
      "rate 450\n",
      "TrainData (1000, 256, 1, 1, 256)\n",
      "TestData (50, 256)\n",
      "ÄnomalyDta (100, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"data\")\n",
    "Dataset.concat_data(\"sample_data\",500)\n",
    "Dataset = dataset.dataset(\"Obrid_AE\", \"test\")\n",
    "print(\"----------------------\")\n",
    "Dataset.concat_data(\"sample_test\",100)\n",
    "print(\"----------------------\")\n",
    "data = Dataset.read_savedata(\"sample_test\")\n",
    "print(data.shape[0])\n",
    "print(\"----------------------\")\n",
    "data, test_data , anomaly_data= Dataset.read_traindata(\"sample_data\", \"sample_test\", 1000, 256, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gl/kcbdb7pj0yz8846ytcdrjm140000gn/T/ipykernel_92309/2313147619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/yukihorikawa/Desktop/LAB_LAST/AutoEncoder/program/syuron/model_data/20211203/newmodel.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrecon_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocsvm_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import dataloader\n",
    "input_model = mainmodel.Autoencoder2()\n",
    "input_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_model = mainmodel.Autoencoder2().to(device)\n",
    "input_model.load_state_dict(torch.load(\"/Users/yukihorikawa/Desktop/LAB_LAST/AutoEncoder/program/syuron/model_data/20211203/newmodel.pth\", map_location=device))\n",
    "\n",
    "recon_list, encoded_list, input_list = dataloader.ocsvm_dataset(input_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_IncompatibleKeys' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gl/kcbdb7pj0yz8846ytcdrjm140000gn/T/ipykernel_91122/2612666299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '_IncompatibleKeys' object is not callable"
     ]
    }
   ],
   "source": [
    "import dataloader\n",
    "num = 1000\n",
    "input_model = mainmodel.Autoencoder2()\n",
    "input_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_model = mainmodel.Autoencoder2().to(device)\n",
    "input_model.load_state_dict(torch.load(\"/Users/yukihorikawa/Desktop/LAB_LAST/AutoEncoder/program/Obrid_AE/model_data/20211202/epoch1000.pth\", map_location=device))\n",
    "# input_model = input_model.to(device)\n",
    "# Dataset.savenumpy(data, test_data, anomaly_data, num)\n",
    "\n",
    "new_model = mainmodel.Modeledit(\"Obrid_AE\").read_model(input_model,\"Obrid_AE/model_data/20211202/epoch1000.pth\")\n",
    "# recon_list, encoded_list, input_list = dataloader.ocsvm_dataset(new_model, data)\n",
    "\n",
    "input_list = []\n",
    "recon_list = []\n",
    "encoded_list = []\n",
    "dataLen = data.shape[0]\n",
    "for i in range(dataLen):\n",
    "    input =  torch.from_numpy((data[i]).astype(np.float32)).clone()\n",
    "    input = input[np.newaxis, np.newaxis, :]\n",
    "    recon = new_model(data).detach().numpy()\n",
    "    encoded = new_model.encoder(input)\n",
    "\n",
    "    input_list.append(input)\n",
    "    recon_list.append(recon)\n",
    "    encoded_list.append(encoded)\n",
    "\n",
    "    if ((dataLen/i)*100) % 10 == 0:\n",
    "        print(\"--------{}/{}--------\".format(i,dataLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gl/kcbdb7pj0yz8846ytcdrjm140000gn/T/ipykernel_91544/219100173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mencoded_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataLen\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------{}/{}--------\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    input_list = []\n",
    "    recon_list = []\n",
    "    encoded_list = []\n",
    "    dataLen = data.shape[0]\n",
    "    for i in range(dataLen):\n",
    "        input =  torch.from_numpy((test_data[i]).astype(np.float32)).clone()\n",
    "        input = input[np.newaxis, np.newaxis, :]\n",
    "        recon = model(input).detach().numpy()\n",
    "        encoded = model.encoder(input)\n",
    "\n",
    "        input_list.append(input)\n",
    "        recon_list.append(recon)\n",
    "        encoded_list.append(encoded)\n",
    "\n",
    "        if ((dataLen/i)*100) % 10 == 0:\n",
    "            print(\"--------{}/{}--------\".format(i,dataLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(anomaly_data.shape[0]):\n",
    "    # if i % 10 == 0:\n",
    "    data0 =  torch.from_numpy((test_data[i]).astype(np.float32)).clone()\n",
    "    test0 =  torch.from_numpy((anomaly_data[i]).astype(np.float32)).clone()\n",
    "    print(type(data0))\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    # data = data0.numpy()\n",
    "    data = data0[np.newaxis, np.newaxis, :]\n",
    "    recon = input_model(data).detach().numpy()\n",
    "    plt.plot(data0.numpy())\n",
    "    plt.plot(recon.flatten())\n",
    "    plt.show()\n",
    "    \n",
    "    anomalypoint=sum(abs(recon.flatten()-data0.numpy()))\n",
    "    print(\"normal:\"+str(anomalypoint))\n",
    "    if(anomalypoint>10):\n",
    "        print(\"anomaly\")\n",
    "        \n",
    "    plt.figure(figsize=(9, 2))\n",
    "    # data = test0.numpy()\n",
    "    data = test0[np.newaxis, np.newaxis, :]\n",
    "    recon = input_model(data).detach().numpy()\n",
    "    plt.plot(test0.numpy(),label=\"testdata\")\n",
    "    plt.plot(recon.flatten(),label=\"recondata\")\n",
    "    plt.show()\n",
    "    \n",
    "    anomalypoint=sum(abs(recon.flatten()-test0.numpy()))\n",
    "    print(\"anomarly:\"+str(anomalypoint))\n",
    "    if(anomalypoint>10):\n",
    "        print(\"anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(anomaly_data.shape[0]):\n",
    "    # if i % 10 == 0:\n",
    "    data0 =  torch.from_numpy((test_data[i]).astype(np.float32)).clone()\n",
    "    test0 =  torch.from_numpy((anomaly_data[i]).astype(np.float32)).clone()\n",
    "    print(type(data0))\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    # data = data0.numpy()\n",
    "    data = data0[np.newaxis, np.newaxis, :]\n",
    "    recon = input_model(data).detach().numpy()\n",
    "    plt.plot(data0.numpy())\n",
    "    plt.plot(recon.flatten())\n",
    "    plt.show()\n",
    "    \n",
    "    anomalypoint=sum(abs(recon.flatten()-data0.numpy()))\n",
    "    print(\"normal:\"+str(anomalypoint))\n",
    "    if(anomalypoint>10):\n",
    "        print(\"anomaly\")\n",
    "        \n",
    "    plt.figure(figsize=(9, 2))\n",
    "    # data = test0.numpy()\n",
    "    data = test0[np.newaxis, np.newaxis, :]\n",
    "    recon = input_model(data).detach().numpy()\n",
    "    plt.plot(test0.numpy(),label=\"testdata\")\n",
    "    plt.plot(recon.flatten(),label=\"recondata\")\n",
    "    plt.show()\n",
    "    \n",
    "    anomalypoint=sum(abs(recon.flatten()-test0.numpy()))\n",
    "    print(\"anomarly:\"+str(anomalypoint))\n",
    "    if(anomalypoint>10):\n",
    "        print(\"anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 256, 1, 1, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[0.0174216 , 0.01567944, 0.01393728, ..., 0.22299652,\n",
       "           0.22125436, 0.2195122 ]]],\n",
       "\n",
       "\n",
       "        [[[0.00174216, 0.00174216, 0.00174216, ..., 0.06968641,\n",
       "           0.06968641, 0.06271777]]],\n",
       "\n",
       "\n",
       "        [[[0.01393728, 0.01045296, 0.01393728, ..., 0.12369338,\n",
       "           0.12543554, 0.12195122]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.01393728, 0.01045296, 0.01393728, ..., 0.12369338,\n",
       "           0.12543554, 0.12195122]]],\n",
       "\n",
       "\n",
       "        [[[0.00348432, 0.00522648, 0.00696864, ..., 0.12195122,\n",
       "           0.12020906, 0.11672474]]],\n",
       "\n",
       "\n",
       "        [[[0.00522648, 0.00348432, 0.00522648, ..., 0.08885017,\n",
       "           0.09059233, 0.08362369]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.02264808, 0.01916376, 0.02264808, ..., 0.14634146,\n",
       "           0.1445993 , 0.1445993 ]]],\n",
       "\n",
       "\n",
       "        [[[0.0174216 , 0.01219512, 0.01219512, ..., 0.21254355,\n",
       "           0.21080139, 0.20905923]]],\n",
       "\n",
       "\n",
       "        [[[0.00348432, 0.00174216, 0.00348432, ..., 0.09059233,\n",
       "           0.09059233, 0.08536585]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.01567944, 0.01393728, 0.01393728, ..., 0.14808362,\n",
       "           0.14634146, 0.14634146]]],\n",
       "\n",
       "\n",
       "        [[[0.00522648, 0.0087108 , 0.00696864, ..., 0.08013937,\n",
       "           0.08013937, 0.08188153]]],\n",
       "\n",
       "\n",
       "        [[[0.03310105, 0.03135889, 0.03484321, ..., 0.16027875,\n",
       "           0.16376307, 0.16027875]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.0261324 , 0.02439024, 0.02264808, ..., 0.09581882,\n",
       "           0.09407666, 0.09407666]]],\n",
       "\n",
       "\n",
       "        [[[0.00696864, 0.01045296, 0.01219512, ..., 0.10452962,\n",
       "           0.10452962, 0.10278746]]],\n",
       "\n",
       "\n",
       "        [[[0.02961672, 0.02787456, 0.02961672, ..., 0.228223  ,\n",
       "           0.228223  , 0.22299652]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.00522648, 0.00348432, 0.00348432, ..., 0.05226481,\n",
       "           0.05052265, 0.05052265]]],\n",
       "\n",
       "\n",
       "        [[[0.0261324 , 0.02439024, 0.02787456, ..., 0.21080139,\n",
       "           0.21080139, 0.20557491]]],\n",
       "\n",
       "\n",
       "        [[[0.00174216, 0.00174216, 0.00522648, ..., 0.07665505,\n",
       "           0.07491289, 0.07317073]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.00522648, 0.00696864, 0.01045296, ..., 0.10452962,\n",
       "           0.10627178, 0.09930314]]],\n",
       "\n",
       "\n",
       "        [[[0.        , 0.00174216, 0.00348432, ..., 0.07665505,\n",
       "           0.07665505, 0.06968641]]],\n",
       "\n",
       "\n",
       "        [[[0.04529617, 0.04355401, 0.04529617, ..., 0.07142857,\n",
       "           0.06794425, 0.06968641]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.00174216, 0.00522648, 0.00696864, ..., 0.11672474,\n",
       "           0.11498258, 0.11498258]]],\n",
       "\n",
       "\n",
       "        [[[0.        , 0.        , 0.00348432, ..., 0.08536585,\n",
       "           0.08362369, 0.07839721]]],\n",
       "\n",
       "\n",
       "        [[[0.00348432, 0.        , 0.00174216, ..., 0.05923345,\n",
       "           0.05923345, 0.05574913]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.00348432, 0.00522648, 0.00696864, ..., 0.09407666,\n",
       "           0.09059233, 0.09059233]]],\n",
       "\n",
       "\n",
       "        [[[0.00696864, 0.00522648, 0.00522648, ..., 0.11149826,\n",
       "           0.10801394, 0.1097561 ]]],\n",
       "\n",
       "\n",
       "        [[[0.00348432, 0.00522648, 0.00696864, ..., 0.09407666,\n",
       "           0.09059233, 0.09059233]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.00174216, 0.00348432, 0.        , ..., 0.06445993,\n",
       "           0.06445993, 0.05749129]]],\n",
       "\n",
       "\n",
       "        [[[0.        , 0.00174216, 0.        , ..., 0.06097561,\n",
       "           0.06097561, 0.06097561]]],\n",
       "\n",
       "\n",
       "        [[[0.00348432, 0.        , 0.00348432, ..., 0.09233449,\n",
       "           0.09059233, 0.08885017]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.00522648, 0.01045296, 0.01045296, ..., 0.03658537,\n",
       "           0.03658537, 0.03484321]]],\n",
       "\n",
       "\n",
       "        [[[0.00522648, 0.00348432, 0.00348432, ..., 0.08536585,\n",
       "           0.08188153, 0.08188153]]],\n",
       "\n",
       "\n",
       "        [[[0.        , 0.00174216, 0.        , ..., 0.07839721,\n",
       "           0.07839721, 0.07491289]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.04006969, 0.03484321, 0.03484321, ..., 0.25609756,\n",
       "           0.25958188, 0.25783972]]],\n",
       "\n",
       "\n",
       "        [[[0.01393728, 0.01045296, 0.01393728, ..., 0.12369338,\n",
       "           0.12543554, 0.12195122]]],\n",
       "\n",
       "\n",
       "        [[[0.00696864, 0.00522648, 0.00522648, ..., 0.08536585,\n",
       "           0.08536585, 0.08188153]]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = dataset.dataset(\"Obrid_AE\", \"test\")\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5f3f9be2ca8d53a2d9e5a69008919cdcaa5ea3bae845fdf455c7915f0fd9345"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('lab2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
